{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Big Data and Spark.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/crystalloide/Spark/blob/master/Big_Data_and_Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfEf-WChi9Yd",
        "colab_type": "text"
      },
      "source": [
        "# Big Data\n",
        "\n",
        "Le Big Data n'est que de la donnée, mais en quantité ?\n",
        "\n",
        "Le Big Data peut être décrit à l'aide des «quatre V» (on évoque même 5 ou 7 V). \n",
        "\n",
        "Les quatre premiers V sont les suivants :\n",
        "1. Volume\n",
        "2. Vitesse\n",
        "3. Variété\n",
        "4. Véracité\n",
        "\n",
        "Les mégadonnées sont décrites comme étant volumineuses, en volume donc (quantité de données : de l'ordre de zétaoctets par exemple), comme arrivant à grande vitesse (données en streaming, par exemple, les données des capteurs ou des téraoctets d'informations commerciales), comme étant de différentes variétés (différentes formes de données, par exemple les vidéos et les tweets) et comme étant d'une véracité plus ou moins fiable, on parle alors d'incertitude de la valeur des données (par exemple, mauvaise qualité des données).\n",
        "\n",
        "En raison de la charge de traitement des mégadonnées, il est nécessaire d'utiliser des outils spéciaux optimisés pour des calculs de cette taille.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UThDq6jri9Yf",
        "colab_type": "text"
      },
      "source": [
        "# Clusters de Stockage et de Calcul\n",
        "\n",
        "Un seul ordinateur ne sera pas en mesure de stocker seul l'ensemble des données à traiter. C'est ce qui est appelé un cluster de stockage : un groupe d'ordinateurs qui se répartissent les fragments de fichiers pour faire stocker la totalité des informations à traiter.\n",
        "\n",
        "L'overhead du traitement des données Big Data  provient du fait qu'un seul ordinateur ne fera pas le travail de traitement des données «big data», mais il faudra faire fonctionner plusieurs ordinateurs ensemble. \n",
        "C'est ce qui est appelé un cluster de calcul : un groupe d'ordinateurs qui travaillent ensemble pour faire le travail attendu.\n",
        "\n",
        "Comment gérer un ensemble d'ordinateurs pour travailler ensemble afin d' accomplir une tâche? \n",
        "\n",
        "Cette gestion du travail en cluster est en réalité difficile à gérer, du fait de la concurrence, de la communication interprocessus, de la planification des tâches, etc. A cela s'ajoute les problèmes \"normaux\" rencontrés sur des systèmes distribués comme les pannes d'ordinateur ou la latence du réseau, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbufdjBii9Yg",
        "colab_type": "text"
      },
      "source": [
        "# Hadoop\n",
        "\n",
        "Pour permettre ces usages liés au Big Data, ** Apache Hadoop ** a été mis au point et est constitué d'un framework, une collection d'outils, destinés à gérer les clusters et les utiliser.\n",
        "\n",
        "- Yarn : gestion des tâches de calcul dans le cluster\n",
        "- HDFS (Hadoop Distributed File System) : stockage des données réparties sur les nœuds du cluster (ordinateurs ou nodes)\n",
        "- Spark : framework permettant de réaliser des calculs sur les données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oe40OMADi9Yh",
        "colab_type": "text"
      },
      "source": [
        "# Débutons avec Spark\n",
        "1. Télécharger [Spark (2.4.3)](https://spark.apache.org/) (ou la version pre-built la plus récente)\n",
        "2. Positionner une variable d'environnement (avec un terminal). Si on utilise  Python 3, cela donne :\n",
        "> export PYSPARK_PYTHON=python3\n",
        "3. Positionner également le path : (chemin d'accès vers les binaires)\n",
        "> export PATH=${PATH}:/home/you/spark-2.4.4-bin-hadoop2.7/bin\n",
        "4. Si vous obtenez une  erreur 'Py4JJavaError', vous devrez installer Java ou OpenJDK version 8\n",
        "\n",
        "Voilà comment configurer Spark.\n",
        "\n",
        "De plus, Pour exécuter un programme spark, il suffit ensuite d'entrer la commande suivante en ligne de commande dans une terminal :\n",
        "\n",
        "> spark-submit spark-program.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUvBFrmSi9Yi",
        "colab_type": "text"
      },
      "source": [
        "# Notre 1er programme Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrwyO75Li9Yi",
        "colab_type": "text"
      },
      "source": [
        "Il faut ensuite cliquer sur \"Ajouter des données\" en haut à droite et de choisir \"Los Angeles Traffic Collision Dataset\".\n",
        "\n",
        "A noter : \n",
        "N'hésitez pas à utiliser un autre dataset lors de futures expériences. \n",
        "Il faut alors vérifiez le type de fichier et changer la méthode de lecture spark en conséquence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "AAX5yjXsi9Yj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "input_dir = '../input'\n",
        "os.listdir(input_dir)\n",
        "file = 'traffic-collision-data-from-2010-to-present.csv'\n",
        "path = os.path.join(input_dir,file)\n",
        "print(path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "F7t1hq7li9Ym",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "5U3C9gaXi9Yo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "from pyspark.sql import SparkSession, functions, types\n",
        " \n",
        "spark = SparkSession.builder.appName('example 1').getOrCreate()\n",
        "spark.sparkContext.setLogLevel('WARN')\n",
        "\n",
        "assert sys.version_info >= (3, 5) # make sure we have Python 3.5+\n",
        "assert spark.version >= '2.3' # make sure we have Spark 2.3+\n",
        "\n",
        "data = spark.read.csv(path, header=True,\n",
        "                      inferSchema=True)\n",
        "data.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ue92JYZgi9Yr",
        "colab_type": "text"
      },
      "source": [
        "Yeah.. it doesn't look pretty. Let's see what we can do. First, we explore some methods with a Spark dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "3-p1Zh1Ci9Yr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# let's see the schema\n",
        "data.printSchema()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "o0O7B7xyi9Yu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# select some columns\n",
        "data.select(data['Crime Code'], data['Victim Age']).show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "YRvJaknii9Yw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# filter the data\n",
        "data.filter(data['Victim Age'] < 40).select('Victim Age', 'Victim Sex').show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "hLuvfipVi9Yz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# write to a json file\n",
        "json_file = data.filter(data['Victim Age'] < 40).select('Victim Age', 'Victim Sex')\n",
        "json_file.write.json('json_output', mode='overwrite')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3gPE9cXi9Y1",
        "colab_type": "text"
      },
      "source": [
        "If you were expecting one json file well no, instead you get a **directory** of multiple json files. The concatenation of those files is the actual output. This is because of the way Spark computes. More on it later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "b8olRMYhi9Y2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls json_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "l2-49tCoi9Y4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# a few more things\n",
        "\n",
        "# perform a calculation on a column and rename it\n",
        "data.select((data['Council Districts']/2).alias('CD_dividedBy2')).show()\n",
        "\n",
        "# rename columns \n",
        "data.withColumnRenamed('Victim Sex', 'Gender').select('Gender').show()\n",
        "\n",
        "# drop columns and a cleaner vertical format for the top 10 \n",
        "d = data.drop('Neighborhood Councils')\n",
        "d.show(n=10, truncate=False, vertical=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ClEJVtki9Y6",
        "colab_type": "text"
      },
      "source": [
        "# Partitioning\n",
        "We previously saw the output of json file is resulted with a directory of multiple json files. This is because we said that big data is too big to be processed on one single computer which is why the Apache Hadoop toolset is there to be able to work with data and compute on multiple computers but can come together as one result as if the data was processed on one machine. This is why all Spark dataframes are partitioned this way no matter how small the data is. \n",
        "\n",
        "Ususally you would give an input directory of files as our \"data\" where each thread/process/core/executor reads an indvidual input file. When creating the output, each write is done in parallel and when each of the output files are combined they form the single output result. That is where HDFS plays a part as the shared filesystem for all of this parallelism to work. \n",
        "\n",
        "YARN is responsbile for managing the computation on each individual computer when actally working with a cluster of nodes. YARN manages the CPU and memory resources. Rather than moving the data to different nodes, YARN can move the compute work to where the data is.\n",
        "\n",
        "On the local machine, we just use the local filesytem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m66VEt7Zi9Y7",
        "colab_type": "text"
      },
      "source": [
        "# Conclusion\n",
        "This was a very breif overview of Big Data and Spark. I am just studying for my final so I thought might as well write about it and share the knowledge with others as a way of studying. If this was useful let me know and I will continue with more details on more PySpark stuff like how it calculates, grouping data and joining data. Also I am not an expert on this stuff so if I am giving some wrong information let me know.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6A9JL52ri9Y8",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "oBZRYfIFi9Y8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}