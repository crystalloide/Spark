{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Big Data and Spark.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/crystalloide/Spark/blob/master/Big_Data_and_Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfEf-WChi9Yd",
        "colab_type": "text"
      },
      "source": [
        "# Big Data\n",
        "\n",
        "Le Big Data n'est que de la donnée, mais en quantité ?\n",
        "\n",
        "Le Big Data peut être décrit à l'aide des «quatre V» (on évoque même 5 ou 7 V). \n",
        "\n",
        "Les quatre premiers V sont les suivants :\n",
        "1. Volume\n",
        "2. Vitesse\n",
        "3. Variété\n",
        "4. Véracité\n",
        "\n",
        "Les mégadonnées sont décrites comme étant volumineuses, en volume donc (quantité de données : de l'ordre de zétaoctets par exemple), comme arrivant à grande vitesse (données en streaming, par exemple, les données des capteurs ou des téraoctets d'informations commerciales), comme étant de différentes variétés (différentes formes de données, par exemple les vidéos et les tweets) et comme étant d'une véracité plus ou moins fiable, on parle alors d'incertitude de la valeur des données (par exemple, mauvaise qualité des données).\n",
        "\n",
        "En raison de la charge de traitement des mégadonnées, il est nécessaire d'utiliser des outils spéciaux optimisés pour des calculs de cette taille.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UThDq6jri9Yf",
        "colab_type": "text"
      },
      "source": [
        "# Clusters de Stockage et de Calcul\n",
        "\n",
        "Un seul ordinateur ne sera pas en mesure de stocker seul l'ensemble des données à traiter. C'est ce qui est appelé un cluster de stockage : un groupe d'ordinateurs qui se répartissent les fragments de fichiers pour faire stocker la totalité des informations à traiter.\n",
        "\n",
        "L'overhead du traitement des données Big Data  provient du fait qu'un seul ordinateur ne fera pas le travail de traitement des données «big data», mais il faudra faire fonctionner plusieurs ordinateurs ensemble. \n",
        "C'est ce qui est appelé un cluster de calcul : un groupe d'ordinateurs qui travaillent ensemble pour faire le travail attendu.\n",
        "\n",
        "Comment gérer un ensemble d'ordinateurs pour travailler ensemble afin d' accomplir une tâche? \n",
        "\n",
        "Cette gestion du travail en cluster est en réalité difficile à gérer, du fait de la concurrence, de la communication interprocessus, de la planification des tâches, etc. A cela s'ajoute les problèmes \"normaux\" rencontrés sur des systèmes distribués comme les pannes d'ordinateur ou la latence du réseau, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbufdjBii9Yg",
        "colab_type": "text"
      },
      "source": [
        "# Hadoop\n",
        "\n",
        "Pour permettre ces usages liés au Big Data, ** Apache Hadoop ** a été mis au point et est constitué d'un framework, une collection d'outils, destinés à gérer les clusters et les utiliser.\n",
        "\n",
        "- Yarn : gestion des tâches de calcul dans le cluster\n",
        "- HDFS (Hadoop Distributed File System) : stockage des données réparties sur les nœuds du cluster (ordinateurs ou nodes)\n",
        "- Spark : framework permettant de réaliser des calculs sur les données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oe40OMADi9Yh",
        "colab_type": "text"
      },
      "source": [
        "# Débutons avec Spark\n",
        "1. Télécharger [Spark (2.4.3)](https://spark.apache.org/) (ou la version pre-built la plus récente)\n",
        "2. Positionner une variable d'environnement (avec un terminal). Si on utilise  Python 3, cela donne :\n",
        "> export PYSPARK_PYTHON=python3\n",
        "3. Positionner également le path : (chemin d'accès vers les binaires)\n",
        "> export PATH=${PATH}:/home/you/spark-2.4.4-bin-hadoop2.7/bin\n",
        "4. Si vous obtenez une  erreur 'Py4JJavaError', vous devrez installer Java ou OpenJDK version 8\n",
        "\n",
        "Voilà comment configurer Spark.\n",
        "\n",
        "De plus, Pour exécuter un programme spark, il suffit ensuite d'entrer la commande suivante en ligne de commande dans une terminal :\n",
        "\n",
        "> spark-submit spark-program.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUvBFrmSi9Yi",
        "colab_type": "text"
      },
      "source": [
        "# Notre 1er programme Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrwyO75Li9Yi",
        "colab_type": "text"
      },
      "source": [
        "Il faut ensuite cliquer sur \"Ajouter des données\" en haut à droite et de choisir \"Los Angeles Traffic Collision Dataset\".\n",
        "\n",
        "A noter : \n",
        "N'hésitez pas à utiliser un autre dataset lors de futures expériences. \n",
        "Il faut alors vérifiez le type de fichier et changer la méthode de lecture spark en conséquence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "AAX5yjXsi9Yj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "input_dir = '../input'\n",
        "os.listdir(input_dir)\n",
        "file = 'traffic-collision-data-from-2010-to-present.csv'\n",
        "path = os.path.join(input_dir,file)\n",
        "print(path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "F7t1hq7li9Ym",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "5U3C9gaXi9Yo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "from pyspark.sql import SparkSession, functions, types\n",
        " \n",
        "spark = SparkSession.builder.appName('example 1').getOrCreate()\n",
        "spark.sparkContext.setLogLevel('WARN')\n",
        "\n",
        "assert sys.version_info >= (3, 5) # make sure we have Python 3.5+\n",
        "assert spark.version >= '2.3' # make sure we have Spark 2.3+\n",
        "\n",
        "data = spark.read.csv(path, header=True,\n",
        "                      inferSchema=True)\n",
        "data.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ue92JYZgi9Yr",
        "colab_type": "text"
      },
      "source": [
        "Hum .. Comme cela n'est pas très joli, voyons ce que l'on peut faire.\n",
        "Tout d'abord, explorons certaines méthodes avec un dataframe Spark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "3-p1Zh1Ci9Yr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Regardons le schéma\n",
        "data.printSchema()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "o0O7B7xyi9Yu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sélectionnons quelques colonnes\n",
        "data.select(data['Crime Code'], data['Victim Age']).show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "YRvJaknii9Yw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Filtrons les données\n",
        "data.filter(data['Victim Age'] < 40).select('Victim Age', 'Victim Sex').show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "hLuvfipVi9Yz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ecrivons dans un fichier en format json\n",
        "json_file = data.filter(data['Victim Age'] < 40).select('Victim Age', 'Victim Sex')\n",
        "json_file.write.json('json_output', mode='overwrite')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3gPE9cXi9Y1",
        "colab_type": "text"
      },
      "source": [
        "Si vous vous attendiez à un simple fichier json, vous allez être surpris.\n",
        "\n",
        "Vous obtenez plutôt un **répertoire** de plusieurs fichiers json. \n",
        "\n",
        "La concaténation de ces fichiers est la sortie réelle. \n",
        "\n",
        "Cela provient de la façon dont Spark calcule. \n",
        "\n",
        "Cela sera détaillé plus tard."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "b8olRMYhi9Y2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls json_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "l2-49tCoi9Y4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encore quelques informations\n",
        "\n",
        "# Opérons un calcul sur une colonne et renommons-là\n",
        "data.select((data['Council Districts']/2).alias('CD_dividedBy2')).show()\n",
        "\n",
        "# Renommons des colonnes \n",
        "data.withColumnRenamed('Victim Sex', 'Gender').select('Gender').show()\n",
        "\n",
        "# Supprimons des colonnes et appliquons un format vertical plus approprié pour les 10 premiers\n",
        "d = data.drop('Neighborhood Councils')\n",
        "d.show(n=10, truncate=False, vertical=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ClEJVtki9Y6",
        "colab_type": "text"
      },
      "source": [
        "# Partitionnement\n",
        "Nous avons vu précédemment que la sortie du fichier json est obtenue avec un répertoire de plusieurs fichiers json. C'est parce que, comme nous l'avons vu également, les (méga)données sont trop volumineuses pour être traitées sur un seul ordinateur.\n",
        "\n",
        "C'est pourquoi les outils fournis dans Apache Hadoop sont là pour : \n",
        "pouvoir travailler avec toutes les données en entrée\n",
        "répartir les calculs sur plusieurs ordinateurs\n",
        "mais aussi rassembler les résultats sous un seul résultat comme si les données avaient été traitées sur une seule machine. \n",
        "C'est pourquoi tous les dataframes Spark sont partitionnés de cette façon, et cela quelle que soit la taille des données.\n",
        "\n",
        "Habituellement, vous indiqueriez un répertoire en entrée contenant vos fichiers de \"données\" où chaque thread / processus / noyau / exécuteur lirait individuellement un fichier d'entrée. \n",
        "Lors de la création de la sortie, chaque écriture est réalisée en parallèle, et lorsque chacun des fichiers de sortie est combiné, ils forment le résultat de sortie unique. C'est là que HDFS joue un rôle -en tant que système de fichiers partagé- pour que tout ce parallélisme fonctionne.\n",
        "\n",
        "YARN est responsable de la gestion du calcul sur chaque ordinateur individuel lorsqu'il travaille effectivement au sein d'un cluster de nœuds. \n",
        "YARN gère les ressources CPU et mémoire. Très important quand on sait que le réseau est une ressource limitante, plutôt que de déplacer les données vers différents nœuds, YARN peut déplacer le travail de calcul là où se trouvent les données.\n",
        "\n",
        "Remaruqe : sur une seule machine locale, nous pouvons utiliser simplement le système de fichiers local"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m66VEt7Zi9Y7",
        "colab_type": "text"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "Ce TP est un 1er et bref aperçu d'un usage de Spark dans un contexte Big Data."
      ]
    }
  ]
}