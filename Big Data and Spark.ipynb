{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Big Data\nBig data is just data but big? Well big data can be described using \"The Four V's\" (other resources have 5 V's or 7 V's). The four V's are as follows: \n1. Volume\n2. Velocity\n3. Variety\n4. Veracity \n\nBig data is described as large in volume (amount of data e.g. zetabytes), high velocity (streaming data e.g. sensor data or terabytes of trade information), coming as a variety (different forms of data, e.g. videos and tweets) and veracity which is the uncertainty of data (e.g. poor data quality).\n\nDue to the processing overhead of big data, we need special tools that are optimized for calculations on this size. "},{"metadata":{},"cell_type":"markdown","source":"# Compute Clusters\n\nPrevious section mention the overhead of processing big data. One computer won't do the job of processing large amounts of data classified as 'big data' but you can have multiple computers work together. This is what a **compute cluster** is, a group of computers that work together to do some work. \n\nOk, so how do we manage to have a group of computers to work together to accomplish a task? This managment of work on clusters is actually hard dealing with concurrency, interprocess communication, scheduling, etc. with the addition of dealing distributed systems problems like computer failures or network latency. "},{"metadata":{},"cell_type":"markdown","source":"# Hadoop\nThankfully we have **Apache Hadoop** is a collection of tools that will assist us for managing clusters. \n\n- Yarn: manages compute jobs in the cluster\n- HDFS: (Hadoop Distributed File System), stores data on the cluster's nodes (computers)\n- Spark: a framework to do computation on the data "},{"metadata":{},"cell_type":"markdown","source":"# Get started with Spark\n1. Download [Spark (2.4.3)](https://spark.apache.org/) (or latest pre-built)\n2. Set an environment variable (e.g. terminal on OS X). I use Python 3 so I did:\n> export PYSPARK_PYTHON=python3\n3. Also set the path:\n> export PATH=${PATH}:/home/you/spark-2.4.4-bin-hadoop2.7/bin\n4. If you run into a 'Py4JJavaError', you may need to install Java or OpenJDK version 8\n\nThese are things I did to set up Spark on my Mac but just Google if these instructions don't work or leave a comment, I can try to help out. Also these instructions are running for spark locally by entering the following in the terminal:\n> spark-submit spark-program.py"},{"metadata":{},"cell_type":"markdown","source":"# A Spark Program"},{"metadata":{},"cell_type":"markdown","source":"I just clicked 'Add Data' at the top right and picked 'Los Angeles Traffic Collision Dataset' so feel free to switch to another dataset when experiementing. Double check the file type though and switch the spark read method accordingly."},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\ninput_dir = '../input'\nos.listdir(input_dir)\nfile = 'traffic-collision-data-from-2010-to-present.csv'\npath = os.path.join(input_dir,file)\nprint(path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pyspark","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nfrom pyspark.sql import SparkSession, functions, types\n \nspark = SparkSession.builder.appName('example 1').getOrCreate()\nspark.sparkContext.setLogLevel('WARN')\n\nassert sys.version_info >= (3, 5) # make sure we have Python 3.5+\nassert spark.version >= '2.3' # make sure we have Spark 2.3+\n\ndata = spark.read.csv(path, header=True,\n                      inferSchema=True)\ndata.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Yeah.. it doesn't look pretty. Let's see what we can do. First, we explore some methods with a Spark dataframe."},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's see the schema\ndata.printSchema()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# select some columns\ndata.select(data['Crime Code'], data['Victim Age']).show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# filter the data\ndata.filter(data['Victim Age'] < 40).select('Victim Age', 'Victim Sex').show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# write to a json file\njson_file = data.filter(data['Victim Age'] < 40).select('Victim Age', 'Victim Sex')\njson_file.write.json('json_output', mode='overwrite')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If you were expecting one json file well no, instead you get a **directory** of multiple json files. The concatenation of those files is the actual output. This is because of the way Spark computes. More on it later."},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls json_output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# a few more things\n\n# perform a calculation on a column and rename it\ndata.select((data['Council Districts']/2).alias('CD_dividedBy2')).show()\n\n# rename columns \ndata.withColumnRenamed('Victim Sex', 'Gender').select('Gender').show()\n\n# drop columns and a cleaner vertical format for the top 10 \nd = data.drop('Neighborhood Councils')\nd.show(n=10, truncate=False, vertical=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Partitioning\nWe previously saw the output of json file is resulted with a directory of multiple json files. This is because we said that big data is too big to be processed on one single computer which is why the Apache Hadoop toolset is there to be able to work with data and compute on multiple computers but can come together as one result as if the data was processed on one machine. This is why all Spark dataframes are partitioned this way no matter how small the data is. \n\nUsusally you would give an input directory of files as our \"data\" where each thread/process/core/executor reads an indvidual input file. When creating the output, each write is done in parallel and when each of the output files are combined they form the single output result. That is where HDFS plays a part as the shared filesystem for all of this parallelism to work. \n\nYARN is responsbile for managing the computation on each individual computer when actally working with a cluster of nodes. YARN manages the CPU and memory resources. Rather than moving the data to different nodes, YARN can move the compute work to where the data is.\n\nOn the local machine, we just use the local filesytem"},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\nThis was a very breif overview of Big Data and Spark. I am just studying for my final so I thought might as well write about it and share the knowledge with others as a way of studying. If this was useful let me know and I will continue with more details on more PySpark stuff like how it calculates, grouping data and joining data. Also I am not an expert on this stuff so if I am giving some wrong information let me know.  "},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}